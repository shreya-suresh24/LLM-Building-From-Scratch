# LLM Building From Scratch
This LLM was built from scratch by making use of transformer architecture based on the 2017 "Attention Is All You Need" paper published by Google, and other functions provided by PyTorch. It is trained on the contents of the book "Anne of Green Gables" and will answer questions corresponding to it.

Built for an internship task during VIT's Summer Research Internship program

Referenced freecodecamp's "Create a Large Language Model from Scratch with Python â€“ Tutorial" for this
